name: Run Bench Main

on:
  workflow_dispatch:
    inputs:
      benchmark_config:
        description: 'Benchmark dataset regex (leave empty for all)'
        required: false
        default: ''
      branches:
        description: 'Space-separated list of branches to benchmark'
        required: false
        default: 'main'
  pull_request:
    branches:
      - main

jobs:
  # Job to generate the matrix configuration
  generate-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Generate matrix
        id: set-matrix
        run: |
          # Print event information for debugging
          echo "Event name: ${{ github.event_name }}"
          echo "Branches input: '${{ github.event.inputs.branches }}'"

          # Default branches based on event type
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "Pull request detected. Using main and PR branch: ${{ github.head_ref }}"
            BRANCHES='["main", "${{ github.head_ref }}"]'
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" && -n "${{ github.event.inputs.branches }}" ]]; then
            # Parse space-separated branches input into JSON array
            echo "Workflow dispatch with branches input detected"
            BRANCHES_INPUT="${{ github.event.inputs.branches }}"
            BRANCHES="["
            for branch in $BRANCHES_INPUT; do
              if [[ "$BRANCHES" != "[" ]]; then
                BRANCHES="$BRANCHES, "
              fi
              BRANCHES="$BRANCHES\"$branch\""
              echo "Adding branch to matrix: $branch"
            done
            BRANCHES="$BRANCHES]"
          else
            echo "Default event type. Using main branch only"
            BRANCHES='["main"]'
          fi

          echo "Generated branches matrix: $BRANCHES"
          echo "matrix={\"jdk\":[24],\"isa\":[\"isa-avx512f\"],\"branch\":$BRANCHES}" >> $GITHUB_OUTPUT

  test-avx512:
    needs: generate-matrix
    concurrency:
      group: ${{ matrix.isa }}-${{ matrix.jdk }}-${{ matrix.branch }}
      cancel-in-progress: false
    strategy:
      matrix: ${{ fromJSON(needs.generate-matrix.outputs.matrix) }}
    runs-on: ${{ matrix.isa }}
    steps:
      - name: verify-avx512
        run: |
          # avx2 is included just for illustration
          required="avx2 avx512f avx512cd avx512bw avx512dq avx512v"
          printf "required ISA feature flags: %s\n" "${required}" 
          flags="$(lscpu|grep '^Flags'|cut -d: -f2)"
          output=""
          for flag in ${required} ; do
           if [[ " $flags " == *"${flag}"* ]]
           then output="${output} $flag(OK)"
           else output="${output} $flag(FAIL)"
          fi ; done
          printf "%s\n" ${output}
          if [[ " $output " == *"FAIL"* ]] ; then exit 2 ; fi
      - name: Set up GCC
        run: |
          sudo apt install -y gcc
      - uses: actions/checkout@v4
      - name: Set up JDK ${{ matrix.jdk }}
        uses: actions/setup-java@v3
        with:
          java-version: ${{ matrix.jdk }}
          distribution: temurin
          cache: maven

      - name: Get version from pom.xml
        id: get-version
        run: |
          VERSION=$(grep -o '<version>[^<]*</version>' pom.xml | head -1 | sed 's/<version>\(.*\)<\/version>/\1/')
          if [[ "$VERSION" == *'${revision}'* ]]; then
            REVISION=$(grep -o '<revision>[^<]*</revision>' pom.xml | head -1 | sed 's/<revision>\(.*\)<\/revision>/\1/')
            if [ -n "$REVISION" ]; then
              VERSION=${VERSION//\$\{revision\}/$REVISION}
            fi
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Current branch has version $VERSION"

      # Print debug information about the current job
      - name: Print job information
        run: |
          echo "Running benchmark for:"
          echo "  - Branch: ${{ matrix.branch }}"
          echo "  - JDK: ${{ matrix.jdk }}"
          echo "  - ISA: ${{ matrix.isa }}"

      # Checkout the branch specified in the matrix
      - name: Checkout branch
        uses: actions/checkout@v4
        with:
          ref: ${{ matrix.branch }}
          fetch-depth: 0

      # Create a directory to store benchmark results
      - name: Create results directory
        run: mkdir -p benchmark_results

      # Build the branch
      - name: Build branch
        run: mvn -B -Punix-amd64-profile package --file pom.xml

      # Run the benchmark if jvector-examples exists
      - name: Run benchmark
        run: |
          # Check if jvector-examples directory and AutoBenchYAML class exist
          if [ ! -d "jvector-examples" ]; then
            echo "Warning: jvector-examples directory not found in branch ${{ matrix.branch }}. Skipping benchmark."
            exit 0
          fi

          # Check if the jar with dependencies was built
          JAR_COUNT=$(ls jvector-examples/target/jvector-examples-*-jar-with-dependencies.jar 2>/dev/null | wc -l)
          if [ "$JAR_COUNT" -eq 0 ]; then
            echo "Warning: No jar with dependencies found in branch ${{ matrix.branch }}. Skipping benchmark."
            exit 0
          fi

          # Determine available memory and set heap size to half of it
          TOTAL_MEM_GB=$(free -g | awk '/^Mem:/ {print $2}')
          # Ensure we have a valid number, default to 16GB total (8GB heap) if detection fails
          if [[ -z "$TOTAL_MEM_GB" ]] || [[ "$TOTAL_MEM_GB" -le 0 ]]; then
            echo "Warning: Could not detect memory size, defaulting to 16GB total memory (8GB heap)"
            TOTAL_MEM_GB=16
          fi
          HALF_MEM_GB=$((TOTAL_MEM_GB / 2))
          # Ensure minimum heap size of 1GB
          if [[ "$HALF_MEM_GB" -lt 1 ]]; then
            HALF_MEM_GB=1
          fi
          echo "Total memory: ${TOTAL_MEM_GB}GB, using ${HALF_MEM_GB}GB for Java heap"
          
          # Run the benchmark
          echo "Running benchmark for branch ${{ matrix.branch }}"
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            java ${{ matrix.jdk >= 20 && '--enable-native-access=ALL-UNNAMED --add-modules=jdk.incubator.vector' || '' }} \
              ${{ matrix.jdk >= 22 && '-Djvector.experimental.enable_native_vectorization=true' || '' }} \
              -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heap_dump/ -Xmx${HALF_MEM_GB}g \
              -cp jvector-examples/target/jvector-examples-*-jar-with-dependencies.jar io.github.jbellis.jvector.example.AutoBenchYAML --output ${{ matrix.branch }}-bench-results dpr-1M
          else
            java ${{ matrix.jdk >= 20 && '--enable-native-access=ALL-UNNAMED --add-modules=jdk.incubator.vector' || '' }} \
              ${{ matrix.jdk >= 22 && '-Djvector.experimental.enable_native_vectorization=true' || '' }} \
              -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heap_dump/ -Xmx${HALF_MEM_GB}g \
              -cp jvector-examples/target/jvector-examples-*-jar-with-dependencies.jar io.github.jbellis.jvector.example.AutoBenchYAML --output ${{ matrix.branch }}-bench-results
          fi

          # Move the results to the benchmark_results directory
          mv ${{ matrix.branch }}-bench-results.csv benchmark_results/ || true
          mv ${{ matrix.branch }}-bench-results.json benchmark_results/ || true

          echo "Completed benchmarks for branch: ${{ matrix.branch }}"

      - name: Upload Individual Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.isa }}-jdk${{ matrix.jdk }}-${{ matrix.branch }}
          path: |
            benchmark_results/*.csv
            benchmark_results/*.json
          if-no-files-found: warn

  # Job to combine results and create visualizations
  combine-results:
    needs: test-avx512
    runs-on: ubuntu-latest
    steps:
      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: all-benchmark-results
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Python Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install argparse matplotlib pandas tabulate psutil

      - name: Combine results and create visualizations
        run: |
          cat > visualize.py << 'EOF'
          import os
          import glob
          import re
          import pandas as pd
          import matplotlib.pyplot as plt
          import subprocess
          import platform
          import psutil
          from datetime import datetime

          # Function to get the most recent commit date for a branch
          def get_branch_commit_date(branch):
              try:
                  # Get the timestamp of the most recent commit on the branch
                  result = subprocess.run(
                      ["git", "show", "-s", "--format=%ct", branch],
                      capture_output=True, text=True, check=True
                  )
                  timestamp = int(result.stdout.strip())
                  return timestamp
              except Exception as e:
                  print(f"Error getting commit date for branch {branch}: {e}")
                  # Return a default timestamp (epoch) for error cases
                  return 0

          # Find all CSV files in the all-benchmark-results directory
          csv_files = glob.glob('all-benchmark-results/**/*-bench-results.csv', recursive=True)

          if not csv_files:
              print("No benchmark results found! Checking other possible locations...")
              csv_files = glob.glob('**/*-bench-results.csv', recursive=True)

          print(f"Found {len(csv_files)} CSV files:")
          for f in csv_files:
              print(f"  - {f}")

          # Track branches and their info
          branches = {}

          # Read and combine all results
          dfs = []
          for file in csv_files:
              try:
                  # Extract branch name from filename
                  filename = os.path.basename(file)
                  branch_match = re.match(r'([^-]+)-bench-results\.csv', filename)
                  branch = branch_match.group(1) if branch_match else "unknown"

                  # Only process each branch once
                  if branch not in branches:
                      # Get commit date for this branch
                      commit_date = get_branch_commit_date(branch)
                      branches[branch] = {
                          'name': branch,
                          'commit_date': commit_date,
                          'datetime': datetime.fromtimestamp(commit_date).strftime('%Y-%m-%d %H:%M:%S')
                      }
                      print(f"Branch {branch} has commit date: {branches[branch]['datetime']}")

                  df = pd.read_csv(file)
                  # Add branch column if not present
                  if 'branch' not in df.columns:
                      df['branch'] = branch

                  dfs.append(df)
                  print(f"Processed {file} with branch {branch}")
              except Exception as e:
                  print(f"Error processing {file}: {e}")

          if not dfs:
              print("No valid benchmark results found!")
              exit(1)

          combined_df = pd.concat(dfs)
          combined_df.to_csv('all_benchmark_results.csv', index=False)
          print(f"Combined {len(dfs)} benchmark results")

          # Sort branches by commit date
          sorted_branches = sorted(branches.keys(), key=lambda b: branches[b]['commit_date'])
          print("Branches in chronological order:")
          for branch in sorted_branches:
              print(f"  - {branch} ({branches[branch]['datetime']})")

          # Create a categorical type with our temporal branch order
          combined_df['branch'] = pd.Categorical(
              combined_df['branch'], 
              categories=sorted_branches, 
              ordered=True
          )
          
          # Sort the dataframe by branch to ensure consistent order
          combined_df = combined_df.sort_values('branch')

          # Add commit dates to the dataframe
          combined_df['commit_date'] = combined_df['branch'].apply(lambda b: branches[b]['datetime'])

          # Get system information
          # Get CPU information
          cpu_count = psutil.cpu_count(logical=False)
          if cpu_count is None:
              cpu_count = psutil.cpu_count(logical=True)
          
          # Get memory information
          memory_gb = round(psutil.virtual_memory().total / (1024**3), 2)
          
          # Get processor information
          processor = platform.processor()
          if not processor:
              processor = platform.machine()
          
          system_info = f"CPU: {processor}, Cores: {cpu_count}, Memory: {memory_gb} GB"
          print(f"System information: {system_info}")

          # Create plots for each metric
          metrics = ['QPS', 'Mean Latency', 'Recall@10']
          for metric in metrics:
              if metric not in combined_df.columns:
                  print(f"Warning: Metric {metric} not found in results")
                  continue

              plt.figure(figsize=(12, 6))

              for dataset, group in combined_df.groupby('dataset'):
                  # Sort the group by our ordered branch column
                  group = group.sort_values('branch')
                  plt.plot(group['branch'], group[metric], marker='o', label=dataset)

              plt.title(f"{metric} Across JVector Branches (Chronological Order)")
              plt.xlabel("Branch")
              plt.ylabel(metric)
              plt.xticks(rotation=45)
              plt.grid(True, linestyle='--', alpha=0.7)
              plt.legend()
              
              # Add system information as text annotation at the bottom of the plot
              plt.figtext(0.5, 0.01, system_info, ha='center', fontsize=8, 
                         bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 5})
              
              plt.tight_layout(rect=[0, 0.03, 1, 1])  # Adjust layout to make room for the annotation

              safe_metric = metric.replace('@', '_at_').replace(' ', '_')
              plt.savefig(f"{safe_metric}.png")
              print(f"Created plot for {metric}")

          # Create a summary markdown report
          with open('benchmark_report.md', 'w') as f:
              f.write("# JVector Branch Benchmark Comparison\n\n")
              f.write(f"Comparing {len(combined_df['branch'].unique())} branches of JVector (ordered by commit date)\n\n")

              f.write("## Branch Timeline\n\n")
              f.write("| Branch | Last Commit Date |\n")
              f.write("|--------|----------------|\n")
              for branch in sorted_branches:
                  f.write(f"| {branch} | {branches[branch]['datetime']} |\n")
              f.write("\n\n")

              f.write("## Summary Table\n\n")
              # Use to_markdown if available, otherwise use to_string
              try:
                  table = combined_df[['branch', 'commit_date', 'dataset'] + [m for m in metrics if m in combined_df.columns]].to_markdown(index=False)
              except AttributeError:
                  table = combined_df[['branch', 'commit_date', 'dataset'] + [m for m in metrics if m in combined_df.columns]].to_string(index=False)
              f.write(table)

              f.write("\n\n## Visualizations\n\n")
              for metric in metrics:
                  if metric not in combined_df.columns:
                      continue
                  safe_metric = metric.replace('@', '_at_').replace(' ', '_')
                  f.write(f"### {metric}\n\n")
                  f.write(f"![{metric} Chart]({safe_metric}.png)\n\n")

          print("Created benchmark report")
          EOF

          python visualize.py

      - name: Upload combined results and visualizations
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-results
          path: |
            all_benchmark_results.csv
            *.png
            benchmark_report.md
          retention-days: 90
